{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WIRIzQTZcJv2"
   },
   "source": [
    "A minimal example of gradient ascent using TF 2.0. Let's begin with an image of clouds, and modify it such that artifacts begin to appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "bqSuiPSbkQna",
    "outputId": "33bd8949-03f4-46e2-87fd-4756b8a56421"
   },
   "outputs": [],
   "source": [
    "!pip install -q tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QCCKOi5k1WY"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "from tensorflow.keras.preprocessing import image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SxxgCLMFeQuq"
   },
   "source": [
    "A few utilities for reading and preprocessing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 644
    },
    "colab_type": "code",
    "id": "17bCHfJXdQjz",
    "outputId": "adf10a7c-5a72-4700-d4e9-da13f780e115"
   },
   "outputs": [],
   "source": [
    "# Download an image and read it into a NumPy array, \n",
    "def download(url):\n",
    "    name = url.split(\"/\")[-1]\n",
    "    image_path = tf.keras.utils.get_file(name, origin=url)\n",
    "    img = image.load_img(image_path)\n",
    "    return image.img_to_array(img)\n",
    "\n",
    "# Scale pixels to between (-1.0 and 1.0)\n",
    "def preprocess(img):\n",
    "    return (img / 127.5) - 1\n",
    "  \n",
    "# Undo the preprocessing above\n",
    "def deprocess(img):\n",
    "    img = img.copy()\n",
    "    img /= 2.\n",
    "    img += 0.5\n",
    "    img *= 255.\n",
    "    return np.clip(img, 0, 255).astype('uint8')\n",
    "\n",
    "# Display an image\n",
    "def show(img):\n",
    "    plt.figure(figsize=(12,12))\n",
    "    plt.grid(False)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)\n",
    "\n",
    "# https://commons.wikimedia.org/wiki/File:Flickr_-_Nicholas_T_-_Big_Sky_(1).jpg\n",
    "url = 'https://storage.googleapis.com/applied-dl/clouds.jpg'\n",
    "img = preprocess(download(url))\n",
    "show(deprocess(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d7f8UCVnjDko"
   },
   "source": [
    "Build a feature extraction model using the [Keras Functional API](https://www.tensorflow.org/alpha/guide/keras/functional). It's pretty sweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "colab_type": "code",
    "id": "PGmA_zG86gB4",
    "outputId": "f32002d1-9b76-4c7e-d886-3d7695954608"
   },
   "outputs": [],
   "source": [
    "inception_v3 = tf.keras.applications.InceptionV3(weights='imagenet',\n",
    "                                                 include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EXSURVuQxIJa"
   },
   "outputs": [],
   "source": [
    "# We'll maximize the activations of these layers\n",
    "names = ['mixed2', 'mixed3', 'mixed4', 'mixed5']\n",
    "layers = [inception_v3.get_layer(name).output for name in names]\n",
    "\n",
    "# Create our feature extraction model\n",
    "feat_extraction_model = tf.keras.Model(inputs=inception_v3.input, outputs=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I8uzM5OwCJai"
   },
   "source": [
    "Here's our forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I3xAgaHE9Qv-"
   },
   "outputs": [],
   "source": [
    "def forward(img):\n",
    "  \n",
    "    # Create a batch\n",
    "    img_batch = tf.expand_dims(img, axis=0)\n",
    "  \n",
    "    # Forward the image through Inception, extract activations\n",
    "    # for the layers we selected above\n",
    "    return feat_extraction_model(img_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHT5Ve_F-M3v"
   },
   "source": [
    "That's it for feature extraction. Now let's define our loss function. It'll be the mean activation of each of the layers we chose above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q_Nrh8thxye5"
   },
   "outputs": [],
   "source": [
    "def calc_loss(layer_activations):\n",
    "  \n",
    "    total_loss = 0\n",
    "  \n",
    "    for act in layer_activations:\n",
    "    \n",
    "    # In gradient ascent, we'll want to maximize this value\n",
    "    # so our image increasingly \"excites\" the layer\n",
    "    loss = tf.math.reduce_mean(act)\n",
    "\n",
    "    # Normalize by the number of units in the layer\n",
    "    loss /= np.prod(act.shape)\n",
    "    total_loss += loss\n",
    "\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tgapTtIx-QLk"
   },
   "source": [
    "We can now run gradient ascent (this is similar to training a classifier, with a twist). We'll treat our input image as a weight matrix, and find the gradients of the loss with respect to it. Those gradients will have the same shape as the image (since there's one for each pixel) - so we can add them directly to the image to modify it. Rinse and repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 589
    },
    "colab_type": "code",
    "id": "Jq11UtV1uMnn",
    "outputId": "0527b31b-1230-4439-a8ec-6b040bf5eb8a"
   },
   "outputs": [],
   "source": [
    "# Convert our image into a variable for training\n",
    "img = tf.Variable(img)\n",
    "\n",
    "# Run a few iterations of gradient ascent\n",
    "steps = 400\n",
    "\n",
    "for step in range(steps):\n",
    "  \n",
    "    with tf.GradientTape() as tape:    \n",
    "        activations = forward(img)\n",
    "        loss = calc_loss(activations)\n",
    "    \n",
    "    # How cool is this? It's the gradient of the \n",
    "    # loss (how excited the layer is) with respect to the\n",
    "    # pixels of our random image!\n",
    "    gradients = tape.gradient(loss, img)\n",
    "\n",
    "    # Normalize the gradients\n",
    "    gradients /= gradients.numpy().std() + 1e-8 \n",
    "  \n",
    "    # Update our image by directly adding the gradients\n",
    "    # (because they're the same shape!)\n",
    "    img.assign_add(gradients)\n",
    "  \n",
    "    if step % 50 == 0:\n",
    "        clear_output()\n",
    "        print (\"Step %d, loss %f\" % (step, loss))\n",
    "        show(deprocess(img.numpy()))\n",
    "        plt.show()\n",
    "\n",
    "# Let's see the result\n",
    "# Notice we're calling .numpy() here, which \n",
    "# takes us from TensorFlow land -> NumPy land\n",
    "clear_output()\n",
    "show(deprocess(img.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v3fAjeKYckyd"
   },
   "source": [
    "That's it! For the bells and whistles (there are many) you can explore this full [implementation](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/examples/tutorials/deepdream) in TensorFlow v1 (perfect for concepts, but you'll probably want to adapt the code to something more modern), and this helpful [notebook](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/8.2-deep-dream.ipynb) from Deep Learning with Python using the Keras reference implementation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "9-deep-dream-minimal.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
